# Deep Reinforcement Learning for Traffic Signal Optimization

## Introduction to DRL in Traffic Signal Control Optimization

Deep Reinforcement Learning (DRL) has emerged as a powerful technique for optimizing various real-world systems. In the realm of traffic engineering, DRL has shown great promise in enhancing traffic signal control. By leveraging the capabilities of deep neural networks and reinforcement learning, DRL agents can learn to make optimal decisions in complex and dynamic traffic environments.

### The Challenges of Traditional Traffic Signal Control

Traditional traffic signal control methods often rely on fixed-time or adaptive control strategies. These methods can be suboptimal in handling varying traffic conditions, especially during peak hours or in the presence of unexpected events.

### The Advantages of DRL in Traffic Signal Control
- Adaptive Control: DRL agents can learn to adapt their control strategies based on real-time traffic conditions, ensuring efficient signal timing.
- Complex Environments: DRL can handle the complexity of urban traffic networks, considering factors such as multiple intersections, varying traffic volumes, and pedestrian crossings.
- Continuous Optimization: DRL allows for continuous improvement of control policies, as agents learn from experience and refine their decision-making process.
- Real-time Decision Making: DRL agents can make decisions in real-time, responding quickly to changes in traffic conditions.

### DRL Framework for Traffic Signal Control

A typical DRL framework for traffic signal control involves the following components:

- State Representation: The agent's state is defined based on the current traffic conditions, such as queue lengths, occupancy rates, and vehicle counts.
- Action Space: The agent's actions are the possible signal timings or phase sequences.
- Reward Function: The reward function defines the goal of the agent. In traffic signal control, it could be minimizing congestion, reducing travel time, or improving fuel efficiency.
- Deep Neural Network: A deep neural network is used to approximate the value function or policy function, which guides the agent's decision-making.
- Reinforcement Learning Algorithm: A reinforcement learning algorithm, such as Q-learning or the Deep Q-Network (DQN), is employed to update the agent's policy based on its experiences.

### Conclusion
DRL offers a promising approach to addressing the challenges of traditional traffic signal control. By leveraging the power of deep learning and reinforcement learning, DRL agents can learn to make intelligent decisions in complex traffic environments, leading to improved traffic efficiency and reduced congestion. As research and development in DRL continue to advance, we can expect to see even more innovative applications in the field of traffic management.

## Fundamental Architecture 

![Architecture](resources/image.png)

- main(): The entry point of the program, likely responsible for initiating other modules and controlling the overall flow.
- init: used for initializing variables, settings, or connections.
- yamls: related to configuration files, perhaps using YAML format for storing settings.
- utils: A utility module, likely providing common functions or tools used by other modules.
-  Agent: related to an agent or entity within the program, such as an AI or a simulated agent.
- save_data: A module for saving data generated by the program.
- traci_ex: a module related to the TraCI (Traffic Control Interface) for SUMO, a traffic simulation software.
- sumo_network: represents the network or environment used in a SUMO simulation.

## 1. [Q-Learning with Epsilon Greedy](./QL-epsilon_greedy/readme.md)

- [source code](QL-epsilon_greedy/)

- Algorithm

```python
if __name__ == '__main__':
    '''
    initialize Agent and learn algorithm related parameters
    '''
    tls_agents = initialize_agents_by(agent_setting,rl_setting)  #
    '''
        start training round
    '''
    for ep in range(basic_setting['episode']):
        print('-------------training round: ', ep, '-------------')
        start_env(sumo_cmd=basic_setting['sumo_start_config_cmd'])  # initialize environment
        prerun_env(pre_steps=basic_setting['pre_steps'])  # environmental preheating
        # start running the simulation environment
        start_time = get_env_time()  # the moment when the simulation environment starts
        while get_env_time() < start_time + basic_setting['simulation_time']:

            for agent in agent_setting.keys():  # traverse all agents
                # obtain the current execution status of the Agent's actions and whether they have been completed
                if tls_agents[agent].is_current_strategy_over() == False:  # the current action is being executed。。。
                    continue  # if the current agent's strategy has not been fully executed, no action will be taken
                # get status
                # get_current_state(agent=tls_agents[agent])
                # select action
                select_action(agent=tls_agents[agent])
                # write actions into the environment
                deploy_action_into_env(agent=tls_agents[agent])
            # perform a one-step simulation
            simulation_step()
            # subtract all Agent policy execution time counters by one
            time_count_step(agent_list=tls_agents)
            # traverse all agents
            for agent in agent_setting.keys():
                # retrieve the execution status of the Agent's actions and confirm if it has been completed
                if tls_agents[agent].is_current_strategy_over() == False:  # the current action is being executed。。。
                    continue  # if the current agent's strategy has not been fully executed, no action will be taken
                # get reward
                get_reward(agent=tls_agents[agent])
                # get state
                get_current_state(agent=tls_agents[agent])
                # update table
                update_q_table(agent=tls_agents[agent])

        # close
        print("this epsido has ended：", ep)
        '''close simulation'''
        close_env()

        '''save the data for this episode'''
        sd.save_ep_data(ep=ep)

        '''save SUMO output data'''
        save_sumo_output_data(ep)

    '''
    run completed, save data to file
    '''
    sd.save_data_to_file()
```

## 2. SARSA with UCB

## 3. Actor-Critic(AC) with boltzmann

## 4. A3C

## 5. NatureDQN-DNN

## 6. DDQN-DNN

## 7. DuelingDQN-DNN

## 8. DuelingDDQN-DNN(D3QN)

## 9. DDPG

## 10. Nash-QL

## 11. Nash-DQN

## 12. Nash-DuelingDQN

## 13. NSHG-QL

## 14. NSHG-DQN

## 15. MFQ-MLP







